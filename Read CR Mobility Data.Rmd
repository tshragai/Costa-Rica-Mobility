---
title: "Costa Rica Mobility Read Data"
output: html_document
date: "2025-10-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r cars}
## upload necessary packages
library(sf)
library(terra)
library(tidyverse)
library(tigris)
library(data.table)
library(leaflet)
library(RColorBrewer)
library(slippymath)
library(scales)
## upload shapefile -- make this whatever polygon you want to extract to could be county or census tract
# california state parks shp (casp)
#casp_shp <- st_read("data/raw/parks/casp_individualpark_boundaries/ParkBoundaries.shp")

setwd("~/Downloads")
```



#Meta Activity Maps
- I only use visit_* tiles (not home_* tiles)
- I only use daytime observations (not nighttime)
##Step 1. Reduce memory burden
- the US csv file was quite large so I had to fiddle around with filtering/making sure my computer didn't explode (play around with this)


```{r}
library(data.table)

## -------- settings you can edit --------
input_dir   <- "~/Documents/CR meta mobility data"     
pattern     <- "\\.csv$"                          # only .csv files
chunk_size  <- 5000
lat_min <- 7.4; lat_max <- 11.4
lon_min <- -86.1; lon_max <- -82.8
country_filter <- "CR"
## --------------------------------------

csv_files <- list.files(input_dir, pattern = pattern, full.names = TRUE)

if (length(csv_files) == 0) {
  stop("No CSV files found in: ", input_dir)
}

# ----------------------------
# process a single CSV
# ----------------------------
process_one <- function(input_file, output_file) {
  # fresh output
  if (file.exists(output_file)) file.remove(output_file)
  
  con <- file(input_file, "r")
  on.exit(close(con), add = TRUE)
  
  header <- readLines(con, n = 1)
  if (length(header) == 0L) return(invisible(NULL))  # empty file
  
  repeat {
    chunk_lines <- readLines(con, n = chunk_size)
    if (length(chunk_lines) == 0L) break
    
    # build a small CSV string with header + this chunk
    chunk <- fread(paste(c(header, chunk_lines), collapse = "\n"))
    
    # filter + append to output
    chunk[
      visit_latitude  >= lat_min & visit_latitude  <= lat_max &
      visit_longitude >= lon_min & visit_longitude <= lon_max &
      country == country_filter
    ][
      , fwrite(.SD, output_file, append = TRUE)
    ]
  }
}

# ===========================================
##single combined output (optional)
# ===========================================
# Uncomment to produce one big file with all filtered rows
 combined_out <- file.path(input_dir, "all_processed_combined.csv")
 if (file.exists(combined_out)) file.remove(combined_out)
 for (infile in csv_files) {
   message("Appending from: ", basename(infile))
   # temp file for this input, then append to combined to keep logic clean
   tmp_out <- tempfile(fileext = ".csv")
   process_one(infile, tmp_out)
   if (file.exists(tmp_out) && file.info(tmp_out)$size > 0) {
     # copy header only once, then append rows
     if (!file.exists(combined_out)) {
       file.copy(tmp_out, combined_out)
     } else {
       # append without header: read tmp and write append
       dt <- fread(tmp_out)
       fwrite(dt, combined_out, append = TRUE)
     }
   }
   unlink(tmp_out)
 }

# ===========================================
# MODE A: per-file outputs (recommended)
# ===========================================
# Each input 'foo.csv' -> 'foo_processed.csv' in the same folder
for (infile in csv_files) {
  outfile <- sub("\\.csv$", "_processed.csv", infile)
  message("Processing: ", basename(infile), " -> ", basename(outfile))
  process_one(infile, outfile)
}
```

read in population data
```{r population data}
setwd("~/Costa-Rica-Mobility/Data")
pop_tiles <- st_read("tile_activity_zoom12_dual_population.geojson")  

#worldpop or GHSL
pop_tiles <- pop_tiles %>%
  mutate(population_dif = worldpop_population - ghsl_population_2025) 

# Map difference in values
ggplot(pop_tiles) +
  geom_sf(aes(fill = population_dif), color = NA) +
  scale_fill_gradient2(
    name = "difference in population",
    midpoint = 0,
    low = "#2c7bb6", mid = "white", high = "#d7191c",
    oob = squish, limits = c(-1000, 1000)   # adjust limits as you like
  ) +
  labs(x = NULL, y = NULL) +
  theme_minimal()

#mmmmm they're pretty different

```




```{r}

## --- Read in activity data ---
setwd("~/Downloads")
ca_data <- read.csv("all_processed_combined.csv")

## set zoom levels, size of tiles around the points. 
#If it gets changed from 13
zoom <- 12L  # may fiddle with this (10 is way too coarse for me)

# pop_tiles is sf with columns: activity, sum, geometry
# 1) rename population col
pop_tiles <- pop_tiles %>% rename(population = ghsl_population_2025)

# 2) get centroids in lon/lat (EPSG:4326) and compute tile indices at your zoom
cent <- st_centroid(st_transform(pop_tiles, 4326))
cc <- st_coordinates(cent)

xy <- map2_dfr(cc[,1], cc[,2], ~{
  t <- lonlat_to_tilenum(.x, .y, zoom)   # your existing helper
  tibble(xtile = as.integer(t$x), ytile = as.integer(t$y))
})

# 3) drop geometry for the join and attach the xtile/ytile
pop_tiles <- bind_cols(st_drop_geometry(pop_tiles), xy)

## compute origin tiles & join origin population 
## NOTE: adjust 'home_longitude'/'home_latitude' if your origin cols are named differently
orig_df <- ca_data %>%
  rowwise() %>%
  mutate(o_tile = list(lonlat_to_tilenum(home_longitude, home_latitude, zoom))) %>%  # <-- origin tiles
  mutate(o_xtile = o_tile$x, o_ytile = o_tile$y) %>%
  ungroup() %>%
  left_join(pop_tiles, by = c("o_xtile" = "xtile", "o_ytile" = "ytile")) %>%
  mutate(origin_pop = ifelse(is.na(population), 0, population))

## summaries daytime activity
## (unchanged filter line kept commented)
activity_summary <- orig_df %>% 
  #filter(day_or_night == "daytime") %>% # may switch to night for heat-ag proj
  group_by(visit_latitude, visit_longitude) %>% # only interested in visit location not home
  ## population-weighted sum over origins visiting each destination point ---
  summarise(activity = sum(visit_fraction * origin_pop, na.rm = TRUE), .groups = "drop")

## compute activity indices for each tile location (destination tiles)
tile_df <- activity_summary %>% 
  rowwise() %>% # need for lonlat_* since its returned as a list per row
  mutate(tile = list(lonlat_to_tilenum(visit_longitude, visit_latitude, zoom))) %>% 
  mutate(xtile = tile$x, ytile = tile$y) %>% # extract x & y index of tile
  ungroup()

## aggregate activity by tile
##  sum (since we already computed weighted contributions), not max ---
tile_activity <- tile_df %>% 
  group_by(xtile, ytile) %>% 
  summarise(activity = sum(activity, na.rm = TRUE), .groups = "drop")

## compute bounding box for each tile
bbox_df <- tile_activity %>% 
  rowwise() %>% 
  mutate(bbox = list(tile_bbox(xtile, ytile, zoom))) %>% 
  mutate(lng1 = bbox$xmin, lng2 = bbox$xmax, # left/right longitudes
         lat1 = bbox$ymin, lat2 = bbox$ymax) %>%  # bottom/top latitudes
  ungroup()

## create tile polygons
polygon_list <- purrr::pmap(list(bbox_df$lng1, bbox_df$lng2, bbox_df$lat1, bbox_df$lat2),
                            function(xmin, xmax, ymin, ymax) {
                              st_polygon(list(matrix(c(
                                 xmin, ymin, # bottom left
                                 xmax, ymin, # bottom right
                                 xmax, ymax, # top right
                                 xmin, ymax, # top left
                                 xmin, ymin), # close polygon
                                 ncol = 2, byrow = TRUE)))
                            })

## create sf object
## If tile_bbox() returns Web Mercator meters, keep crs = 3857 then transform; 
## if it returns lon/lat, set crs = 4326 directly.
tile_activity_sf <- st_sf(activity = bbox_df$activity,
                          geometry = st_sfc(polygon_list, crs = 3857)) # keep as in your code
# convert 3857 to 4326 for leaflet & downstream analysis
tile_activity_lonlat <- st_transform(tile_activity_sf, crs = 4326) 

## save output (unchanged file targets; rename if you want to distinguish weighted)
if (!dir.exists("data")) dir.create("data")
st_write(tile_activity_lonlat, "data/tile_activity_lonlat_zoom12.gpkg", delete_dsn = TRUE)
st_write(tile_activity_lonlat, "data/tile_activity_lonlat_zoom12.shp", delete_layer = TRUE)
st_write(tile_activity_lonlat, "data/tile_activity_lonlat_zoom12.geojson", delete_dsn = TRUE)
```




##Step 3. Plot it in Leaflet
```{r}

tile_activity_lonlat<-tile_activity_lonlat%>%
  mutate(activity_log=log(activity+1))

# create color scale
pal <- colorNumeric("YlOrRd", domain = tile_activity_lonlat$activity_log, na.color = "transparent")
## create leaflet map
leaflet(tile_activity_lonlat) %>% 
  addTiles() %>% 
  addPolygons(fillColor = ~pal(activity_log),
              weight = 1, color = "black", fillOpacity = 0.7,
              popup = ~paste0("Activity", round(activity, 3))) %>% 
  addLegend(pal = pal, values = ~activity, title = "Activity Level", position = "bottomright")

# create color scale
pal <- colorNumeric("YlOrRd", domain = tile_activity_lonlat$activity, na.color = "transparent")
## create leaflet map
leaflet(tile_activity_lonlat) %>% 
  addTiles() %>% 
  addPolygons(fillColor = ~pal(activity),
              weight = 1, color = "black", fillOpacity = 0.7,
              popup = ~paste0("Activity", round(activity, 3))) %>% 
  addLegend(pal = pal, values = ~activity, title = "Activity Level", position = "bottomright")


#view the underlying movement data
visit_counts <- ca_data %>%
  count(visit_latitude, visit_longitude, name = "n_visits")
 visit_points <- st_as_sf(
     visit_counts,
     coords = c("visit_longitude", "visit_latitude"),
     crs = 4326  # WGS84 lat/lon
 )
pal <- colorNumeric("YlOrRd", domain = log(visit_points$n_visits+1))

leaflet(visit_points) %>%
  addTiles() %>%
  addCircleMarkers(
    radius = 4,
    stroke = FALSE,
    fillOpacity = 0.7,
    color = ~pal(log(n_visits+1)),
    popup = ~paste0("Visits: ", log(n_visits+1))
  ) %>%
  addLegend(
    pal = pal,
    values = ~n_visits,
    title = "Number of visits",
    position = "bottomright"
  )
 

```

```{r}


```




##Step 4. Summarize and merge with polygon boundary
- your CRS might be different (play around with st_valid() etc)
```{r}
#read in Costa Rica district data 
setwd("~/Downloads/CR_district_population_GHSL_2025")
districts <- st_read("CR_district_population_GHSL_2025.shp") %>%
  st_make_valid()

# Pick the column that identifies each district (edit this if different)
name_col <- "NOMB_UGED"   # e.g., "DISTRITO", "NAME", etc.

# Make sure districts share CRS with tiles for mapping
districts <- st_transform(districts, st_crs(tile_activity_lonlat))

# --- 2) Compute totals per district using area-aware overlay ---
# Work in a projected CRS for correct area math
tiles_m   <- st_transform(tile_activity_lonlat, 3857) %>%
  mutate(
    tile_area_m2     = as.numeric(st_area(geometry)),
    activity_density = activity / tile_area_m2   # activity per m^2
  )

districts_m <- st_transform(districts, 3857) %>%
  mutate(district_area_m2 = as.numeric(st_area(geometry)))

# Intersect tiles and districts
ix <- st_intersection(
  tiles_m %>% select(activity_density),
  districts_m %>% select(all_of(name_col), district_area_m2)
) %>%
  mutate(ix_area_m2 = as.numeric(st_area(geometry)))

# Summarize per district
district_activity <- ix %>%
  st_drop_geometry() %>%
  group_by(.data[[name_col]]) %>%
  summarise(
    total_activity           = sum(activity_density * ix_area_m2, na.rm = TRUE),        # ∑ density × area
    mean_activity_density_aw = total_activity / first(district_area_m2),                # area-weighted mean
    max_tile_density         = max(activity_density, na.rm = TRUE),                     # optional: max density tile overlapping district
    sd_tile_density          = sd(activity_density,  na.rm = TRUE),                     # optional: sd of tile densities
    .groups = "drop"
  )

# --- 3) Join back to polygons for mapping/exports ---
districts_with_activity <- districts %>%
  left_join(district_activity, by = setNames(name_col, name_col))

# Example: map with leaflet (uses lon/lat CRS)
pal <- colorNumeric("YlOrRd", domain = log(districts_with_activity$total_activity+1))
 leaflet(districts_with_activity) %>%
   addTiles() %>%
   addPolygons(fillColor = ~pal(log(total_activity+1)), color = "#333", weight = 0.5,
               fillOpacity = 0.7
               ) 


 
 
 # log-transform then stretch domain
vals <- log1p(districts_with_activity$total_activity)
pal <- colorNumeric(
  palette = "YlOrRd",
  domain  = range(vals, na.rm = TRUE),
  na.color = "transparent"
)

leaflet(districts_with_activity) %>%
  addTiles() %>%
  addPolygons(
    fillColor   = ~pal(log1p(total_activity)),
    color       = "#333",
    weight      = 0.5,
    fillOpacity = 0.8
  ) %>%
  addLegend(
    pal     = pal,
    values  = vals,
    title   = "log(total activity + 1)",
    position = "bottomright"
  )

vals <- log1p(districts_with_activity$total_activity)
pal <- colorBin(
  palette = "YlOrRd",
  domain  = vals,
  bins    = 7,          # fewer bins = bigger visible jumps
  na.color = "transparent"
)

pal <- colorNumeric(
  palette = "viridis",   # or "magma", "plasma", "viridis"
  domain  = log1p(districts_with_activity$total_activity),
  na.color = "transparent"
)
```


```{r pressure, echo=FALSE}
# ===========================================================
# District-to-District Activity Data 
# ===========================================================

# ---- 3) Compute origin tile & origin_pop on same grid ----
orig_df <- ca_data %>%
  rowwise() %>%
  mutate(o_tile = list(slippymath::lonlat_to_tilenum(home_longitude, home_latitude, zoom))) %>%
  mutate(o_xtile = o_tile$x, o_ytile = o_tile$y) %>%
  ungroup() %>%
  left_join(pop_tiles, by = c("o_xtile" = "xtile", "o_ytile" = "ytile")) %>%
  mutate(origin_pop = ifelse(is.na(population), 0, population))

# ---- 4) Destination tile indices & tile->tile weighted flows ----
od_tiles <- orig_df %>%
  rowwise() %>%
  mutate(d_tile = list(slippymath::lonlat_to_tilenum(visit_longitude, visit_latitude, zoom))) %>%
  mutate(d_xtile = d_tile$x, d_ytile = d_tile$y) %>%
  ungroup() %>%
  select(o_xtile, o_ytile, d_xtile, d_ytile, visit_fraction, origin_pop)

tile_flows <- od_tiles %>%
  group_by(o_xtile, o_ytile, d_xtile, d_ytile) %>%
  summarise(weight = sum(visit_fraction * origin_pop, na.rm = TRUE), .groups = "drop") %>%
  filter(weight > 0)

# ---- 5) Build tile polygons correctly (CRS fix) ----

# tiles we need (both ends)
tiles_needed <- bind_rows(
  tile_flows %>% transmute(xtile = o_xtile, ytile = o_ytile),
  tile_flows %>% transmute(xtile = d_xtile, ytile = d_ytile)
) %>% distinct()

# helper: polygon from bbox
poly_from_bbox <- function(xmin, ymin, xmax, ymax) {
  st_polygon(list(matrix(
    c(xmin,ymin,  xmax,ymin,  xmax,ymax,  xmin,ymax,  xmin,ymin),
    ncol = 2, byrow = TRUE
  )))
}

# collect bboxes
bboxes <- pmap(tiles_needed, function(xtile, ytile) {
  b <- tile_bbox(xtile, ytile, zoom)  # your helper; must return xmin/xmax (lon or meters), ymin/ymax (lat or meters)
  list(xtile = xtile, ytile = ytile, xmin = b$xmin, ymin = b$ymin, xmax = b$xmax, ymax = b$ymax)
})

bbox_df <- as_tibble(do.call(rbind, lapply(bboxes, as.data.frame)))

# detect bbox units -> set creation CRS
is_lonlat <- with(bbox_df, max(abs(c(xmin, xmax))) <= 180 && max(abs(c(ymin, ymax))) <= 90)
crs_bbox  <- if (is_lonlat) 4326 else 3857

# build sfc once
poly_list <- pmap(bbox_df[, c("xmin","ymin","xmax","ymax")], poly_from_bbox)
tiles_raw <- st_sf(
  xtile = bbox_df$xtile,
  ytile = bbox_df$ytile,
  geometry = st_sfc(poly_list, crs = crs_bbox)
)
stopifnot(!any(st_is_empty(tiles_raw)))

# project to equal-area CRS for area math
tiles_m <- st_transform(tiles_raw, crs_area) %>%
  st_make_valid() %>%
  mutate(tile_area_m2 = as.numeric(st_area(geometry)))
stopifnot(!any(st_is_empty(tiles_m)))

# ---- 6) Read districts & project to same area CRS ----
districts_ll <- districts
if (is.na(st_crs(districts_ll))) st_crs(districts_ll) <- 4326
stopifnot(name_col %in% names(districts_ll))

districts_m <- st_transform(districts_ll, crs_area) %>%
  st_make_valid()
stopifnot(!any(st_is_empty(districts_m)))

# ---- 7) Tile→District area fractions (for areal split) ----
ix <- st_intersection(
  tiles_m %>% select(xtile, ytile, tile_area_m2),
  districts_m %>% select(all_of(name_col))
) %>%
  mutate(ix_area_m2 = as.numeric(st_area(geometry)),
         frac = ifelse(tile_area_m2 > 0, ix_area_m2 / tile_area_m2, 0)) %>%
  st_drop_geometry() %>%
  rename(district = !!name_col)

tile2dist <- ix %>% select(xtile, ytile, district, frac)

# ---- 8) Distribute tile->tile flows into district->district flows ----
# join origin fractions
flows_o <- tile_flows %>%
  left_join(tile2dist, by = c("o_xtile" = "xtile", "o_ytile" = "ytile")) %>%
  rename(o_district = district, o_frac = frac) %>%
  filter(!is.na(o_district), o_frac > 0)

# join destination fractions
flows_od <- flows_o %>%
  left_join(tile2dist, by = c("d_xtile" = "xtile", "d_ytile" = "ytile")) %>%
  rename(d_district = district, d_frac = frac) %>%
  filter(!is.na(d_district), d_frac > 0)

# final district->district flows (TOTALS, partial tiles handled)
dist2dist <- flows_od %>%
  mutate(flow = weight * o_frac * d_frac) %>%
  group_by(o_district, d_district) %>%
  summarise(flow = sum(flow, na.rm = TRUE), .groups = "drop") %>%
  filter(flow > 0)

```

