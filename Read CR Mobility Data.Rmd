---
title: "Costa Rica Mobility Read Data"
output: html_document
date: "2025-10-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r cars}
## upload necessary packages
library(sf)
library(terra)
library(tidyverse)
library(tigris)
library(data.table)
library(leaflet)
library(RColorBrewer)
library(slippymath)
library(scales)
## upload shapefile -- make this whatever polygon you want to extract to could be county or census tract
# california state parks shp (casp)
#casp_shp <- st_read("data/raw/parks/casp_individualpark_boundaries/ParkBoundaries.shp")

setwd("~/Downloads")
```



#Meta Activity Maps
- I only use visit_* tiles (not home_* tiles)
- I only use daytime observations (not nighttime)
##Step 1. Reduce memory burden
- the US csv file was quite large so I had to fiddle around with filtering/making sure my computer didn't explode (play around with this)


```{r}
library(data.table)

## -------- settings you can edit --------
input_dir   <- "~/Documents/CR meta mobility data"     
pattern     <- "\\.csv$"                          # only .csv files
chunk_size  <- 5000
lat_min <- 7.4; lat_max <- 11.4
lon_min <- -86.1; lon_max <- -82.8
country_filter <- "CR"
## --------------------------------------

csv_files <- list.files(input_dir, pattern = pattern, full.names = TRUE)

if (length(csv_files) == 0) {
  stop("No CSV files found in: ", input_dir)
}

# ----------------------------
# process a single CSV
# ----------------------------
process_one <- function(input_file, output_file) {
  # fresh output
  if (file.exists(output_file)) file.remove(output_file)
  
  con <- file(input_file, "r")
  on.exit(close(con), add = TRUE)
  
  header <- readLines(con, n = 1)
  if (length(header) == 0L) return(invisible(NULL))  # empty file
  
  repeat {
    chunk_lines <- readLines(con, n = chunk_size)
    if (length(chunk_lines) == 0L) break
    
    # build a small CSV string with header + this chunk
    chunk <- fread(paste(c(header, chunk_lines), collapse = "\n"))
    
    # filter + append to output
    chunk[
      visit_latitude  >= lat_min & visit_latitude  <= lat_max &
      visit_longitude >= lon_min & visit_longitude <= lon_max &
      country == country_filter
    ][
      , fwrite(.SD, output_file, append = TRUE)
    ]
  }
}

# ===========================================
##single combined output (optional)
# ===========================================
# Uncomment to produce one big file with all filtered rows
 combined_out <- file.path(input_dir, "all_processed_combined.csv")
 if (file.exists(combined_out)) file.remove(combined_out)
 for (infile in csv_files) {
   message("Appending from: ", basename(infile))
   # temp file for this input, then append to combined to keep logic clean
   tmp_out <- tempfile(fileext = ".csv")
   process_one(infile, tmp_out)
   if (file.exists(tmp_out) && file.info(tmp_out)$size > 0) {
     # copy header only once, then append rows
     if (!file.exists(combined_out)) {
       file.copy(tmp_out, combined_out)
     } else {
       # append without header: read tmp and write append
       dt <- fread(tmp_out)
       fwrite(dt, combined_out, append = TRUE)
     }
   }
   unlink(tmp_out)
 }

# ===========================================
# MODE A: per-file outputs (recommended)
# ===========================================
# Each input 'foo.csv' -> 'foo_processed.csv' in the same folder
for (infile in csv_files) {
  outfile <- sub("\\.csv$", "_processed.csv", infile)
  message("Processing: ", basename(infile), " -> ", basename(outfile))
  process_one(infile, outfile)
}
```

read in population data
```{r population data}
setwd("~/Costa-Rica-Mobility/Data")
pop_tiles <- st_read("tile_activity_dual_population.geojson")  

#worldpop or GHSL
pop_tiles <- pop_tiles %>%
  mutate(population_dif = worldpop_population - ghsl_population_2025) 

# Map difference in values
ggplot(pop_tiles) +
  geom_sf(aes(fill = population_dif), color = NA) +
  scale_fill_gradient2(
    name = "difference in population",
    midpoint = 0,
    low = "#2c7bb6", mid = "white", high = "#d7191c",
    oob = squish, limits = c(-1000, 1000)   # adjust limits as you like
  ) +
  labs(x = NULL, y = NULL) +
  theme_minimal()

#mmmmm they're pretty different

```




```{r}

## --- NEW: read both inputs ---
setwd("~/Downloads")
ca_data <- read.csv("all_processed_combined.csv")

pop_tiles <- geojsonsf::geojson_sf(
  pop_tiles$geometry  # vector of GeoJSON strings
) %>%
  # geojson_sf returns an sf with geometry but drops other columns, so join back:
  bind_cols(pop_tiles %>% select(-geometry))

## --- moved up (so we can use it to tile origins) ---
zoom <- 13  # may fiddle with this (10 is way too coarse for me)

# pop_tiles is sf with columns: activity, sum, geometry
# 1) rename population col
pop_tiles <- pop_tiles %>% rename(population = sum)

# 2) get centroids in lon/lat (EPSG:4326) and compute tile indices at your zoom
cent <- st_centroid(st_transform(pop_tiles, 4326))
cc <- st_coordinates(cent)

xy <- map2_dfr(cc[,1], cc[,2], ~{
  t <- lonlat_to_tilenum(.x, .y, zoom)   # your existing helper
  tibble(xtile = as.integer(t$x), ytile = as.integer(t$y))
})

# 3) drop geometry for the join and attach the xtile/ytile
pop_tiles <- bind_cols(st_drop_geometry(pop_tiles), xy)

## --- NEW: compute origin tiles & join origin population ---
## NOTE: adjust 'home_longitude'/'home_latitude' if your origin cols are named differently
orig_df <- ca_data %>%
  rowwise() %>%
  mutate(o_tile = list(lonlat_to_tilenum(home_longitude, home_latitude, zoom))) %>%  # <-- origin tiles
  mutate(o_xtile = o_tile$x, o_ytile = o_tile$y) %>%
  ungroup() %>%
  left_join(pop_tiles, by = c("o_xtile" = "xtile", "o_ytile" = "ytile")) %>%
  mutate(origin_pop = ifelse(is.na(population), 0, population))

## summaries daytime activity
## (unchanged filter line kept commented)
activity_summary <- orig_df %>% 
  #filter(day_or_night == "daytime") %>% # may switch to night for heat-ag proj
  group_by(visit_latitude, visit_longitude) %>% # only interested in visit location not home
  ## --- CHANGED: population-weighted sum over origins visiting each destination point ---
  summarise(activity = sum(visit_fraction * origin_pop, na.rm = TRUE), .groups = "drop")

## compute activity indices for each tile location (destination tiles)
tile_df <- activity_summary %>% 
  rowwise() %>% # need for lonlat_* since its returned as a list per row
  mutate(tile = list(lonlat_to_tilenum(visit_longitude, visit_latitude, zoom))) %>% 
  mutate(xtile = tile$x, ytile = tile$y) %>% # extract x & y index of tile
  ungroup()

## aggregate activity by tile
## --- CHANGED: sum (since we already computed weighted contributions), not max ---
tile_activity <- tile_df %>% 
  group_by(xtile, ytile) %>% 
  summarise(activity = sum(activity, na.rm = TRUE), .groups = "drop")

## compute bounding box for each tile
bbox_df <- tile_activity %>% 
  rowwise() %>% 
  mutate(bbox = list(tile_bbox(xtile, ytile, zoom))) %>% 
  mutate(lng1 = bbox$xmin, lng2 = bbox$xmax, # left/right longitudes
         lat1 = bbox$ymin, lat2 = bbox$ymax) %>%  # bottom/top latitudes
  ungroup()

## create tile polygons
polygon_list <- purrr::pmap(list(bbox_df$lng1, bbox_df$lng2, bbox_df$lat1, bbox_df$lat2),
                            function(xmin, xmax, ymin, ymax) {
                              st_polygon(list(matrix(c(
                                 xmin, ymin, # bottom left
                                 xmax, ymin, # bottom right
                                 xmax, ymax, # top right
                                 xmin, ymax, # top left
                                 xmin, ymin), # close polygon
                                 ncol = 2, byrow = TRUE)))
                            })

## create sf object
## If tile_bbox() returns Web Mercator meters, keep crs = 3857 then transform; 
## if it returns lon/lat, set crs = 4326 directly.
tile_activity_sf <- st_sf(activity = bbox_df$activity,
                          geometry = st_sfc(polygon_list, crs = 3857)) # keep as in your code
# convert 3857 to 4326 for leaflet & downstream analysis
tile_activity_lonlat <- st_transform(tile_activity_sf, crs = 4326) 

## save output (unchanged file targets; rename if you want to distinguish weighted)
if (!dir.exists("data")) dir.create("data")
st_write(tile_activity_lonlat, "data/tile_activity_lonlat.gpkg", delete_dsn = TRUE)
st_write(tile_activity_lonlat, "data/tile_activity_lonlat.shp", delete_layer = TRUE)
st_write(tile_activity_lonlat, "data/tile_activity_lonlat.geojson", delete_dsn = TRUE)
```




##Step 3. Plot it in Leaflet
```{r}

tile_activity_lonlat<-tile_activity_lonlat%>%
  mutate(activity_log=log(activity+1))

# create color scale
pal <- colorNumeric("YlOrRd", domain = tile_activity_lonlat$activity_log, na.color = "transparent")
## create leaflet map
leaflet(tile_activity_lonlat) %>% 
  addTiles() %>% 
  addPolygons(fillColor = ~pal(activity_log),
              weight = 1, color = "black", fillOpacity = 0.7,
              popup = ~paste0("Activity", round(activity, 3))) %>% 
  addLegend(pal = pal, values = ~activity, title = "Activity Level", position = "bottomright")


library(leaflet)

pop_tiles<-pop_tiles%>%
  mutate(sum_log=log(sum+1))

pal <- colorNumeric("YlGnBu", domain = pop_tiles$sum_log, na.color = "transparent")

leaflet(pop_tiles) %>% 
  addTiles() %>% 
  addPolygons(
    fillColor = ~pal(sum_log),
    color = "black", weight = 0.5, fillOpacity = 0.7,
    popup = ~paste0("Population: ", round(sum_log))
  ) %>%
  addLegend(
    pal = pal, values = ~sum_log,
    title = "Population",
    position = "bottomright"
  )

```
##Step 4. Summarize and merge with polygon boundary
- your CRS might be different (play around with st_valid() etc)
```{r}
# fix my polygons
casp_shp_clean_meta1 <- casp_shp_clean %>% st_make_valid()
casp_shp_clean_meta <- st_transform(casp_shp_clean_meta1, crs = st_crs(tile_activity_lonlat))
# find intersection bwn meta data & park polygons
activity_casp <- st_intersection(activitymap_us1, casp_shp_clean_meta)
# summarise activity data
activity_casp_summary <- activity_casp %>% 
  group_by(UNITNAME) %>% 
  summarise(mean_activity = mean(activity, na.rm = TRUE),
            max_activity = max(activity, na.rm = TRUE), 
            sd_activity = sd(activity, na.rm = TRUE), .groups = "drop") %>% 
  st_drop_geometry()
# merge data with park data
casp_meta_activity <- casp_shp_clean  %>% 
  left_join(activity_casp_summary, by = "UNITNAME")
## ^^ this is my final dataset that I'm building with other covariates

```


```{r pressure, echo=FALSE}

```

